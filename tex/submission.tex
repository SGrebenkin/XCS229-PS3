% This contents of this file will be inserted into the _Solutions version of the
% output tex document.  Here's an example:

% If assignment with subquestion (1.a) requires a written response, you will
% find the following flag within this document: <SCPD_SUBMISSION_TAG>_1a
% In this example, you would insert the LaTeX for your solution to (1.a) between
% the <SCPD_SUBMISSION_TAG>_1a flags.  If you also constrain your answer between the
% START_CODE_HERE and END_CODE_HERE flags, your LaTeX will be styled as a
% solution within the final document.

% Please do not use the '<SCPD_SUBMISSION_TAG>' character anywhere within your code.  As expected,
% that will confuse the regular expressions we use to identify your solution.
\def\assignmentnum{2 }
\def\assignmenttitle{XCS229 Problem Set \assignmentnum}
\input{macros}
\begin{document}
\pagestyle{myheadings} \markboth{}{\assignmenttitle}

% <SCPD_SUBMISSION_TAG>_entire_submission

This handout includes space for every question that requires a written response.
Please feel free to use it to handwrite your solutions (legibly, please).  If
you choose to typeset your solutions, the |README.md| for this assignment includes
instructions to regenerate this handout with your typeset \LaTeX{} solutions.
\ruleskip

\LARGE
1.a
\normalsize

% <SCPD_SUBMISSION_TAG>_1a
\begin{answer}
  Since $g'(z) = g(z)(1-g(z))$ and $h(x) = g(\theta^T x)$, it follows that $\partial h(x) / \partial \theta_k = h(x)(1 - h(x)) x_k$.

  Letting $h_{\theta}(x^{(i)}) = g(\theta^T x^{(i)})
  = 1/(1 + \exp(-\theta^T x^{(i)}))$, we have\\

  \begin{flalign*}
    \frac{\partial\log h_{\theta}(x^{(i)})}{\partial\theta_k} = 
    % ### START CODE HERE ###
    \frac{\partial\log h_{\theta}(x^{(i)})}{\partial h_{\theta}(x^{(i)})} \frac{\partial h_{\theta}(x^{(i)})}{\partial\theta_k} = \frac{1}{h_{\theta}(x^{(i)})} \frac{\partial g(\theta^Tx^{(i)})}{\partial \theta_k} = \frac{x^{(i)}_k g(\theta^Tx^{(i)})(1 - g(\theta^Tx^{(i)}))}{h_{\theta}(x^{(i)})} =  x^{(i)}_k (1 - h_{\theta}(x^{(i)})) \\
    % ### END CODE HERE ###
    \frac{\partial\log(1 - h_{\theta}(x^{(i)}))}{\partial\theta_k} =
    % ### START CODE HERE ###
    -\frac{1}{(1-h_{\theta}(x^{(i)}))} \frac{\partial h_{\theta}(x^{(i)})}{\partial \theta_k} = -\frac{1}{(1-h_{\theta}(x^{(i)}))} \frac{\partial g(\theta^Tx^{(i)})}{\partial \theta_k} = -\frac{x^{(i)}_k g(\theta^Tx^{(i)})(1 - g(\theta^Tx^{(i)}))}{(1-h_{\theta}(x^{(i)}))}=-x^{(i)}_k h_{\theta}(x^{(i)})
    % ### END CODE HERE ###
  \end{flalign*}

  Substituting into our equation for $J(\theta)$, we have
  %
  \begin{flalign*}
    \frac{\partial J(\theta)}{\partial\theta_k} =
    % ### START CODE HERE ###
    \frac{1}{n} \sum_{i=1}^{n} y^{(i)}\frac{\partial log(h_{\theta}(x^{(i)}))}{\partial\theta_k} + (1 - y^{(i)})\frac{\partial log(1 - h_{\theta}(x^{(i)}))}{\partial \theta_k} = \\
    = \frac{1}{n} \sum_{i=1}^{n} y^{(i)}x^{(i)}_k (1 - h_{\theta}(x^{(i)}))-(1-y^{(i)})x^{(i)}_k h_{\theta}(x^{(i)}) = -\frac{1}{n} \sum_{i=1}^{n} (y^{(i)} - h_{\theta}(x^{(i)}))x^{(i)}_k
    % ### END CODE HERE ###
  \end{flalign*}
  
  Consequently, the $(k, l)$ entry of the Hessian is given by
  
  \begin{flalign*}
    H_{kl} = \frac{\partial^2 J(\theta)}{\partial\theta_k\partial\theta_l} = 
    % ### START CODE HERE ###
    -\frac{1}{n} \sum_{i=1}^{n} \frac{\partial (y^{(i)} - h_{\theta}(x^{(i)}))x^{(i)}_k}{\partial \theta_l} = \frac{1}{n} \sum_{i=1}^{n} x^{(i)}_k \frac{\partial h_{\theta}(x^{(i)})}{\partial \theta_l} = \frac{1}{n} \sum_{i=1}^{n}  h_{\theta}(x^{(i)})(1 - h_{\theta}(x^{(i)})) x^{(i)}_k x^{(i)}_l
    % ### END CODE HERE ###
  \end{flalign*}
  
  Using the fact that $X_{ij} = x_i x_j$ if and only if $X = xx^T$, we have
  
  \begin{flalign*}
    H = 
    % ### START CODE HERE ###
    \frac{1}{n} \sum_{i=1}^{n} h_{\theta}(x^{(i)})(1 - h_{\theta}(x^{(i)})) x^{(i)} x^{(i)T} = 
    % ### END CODE HERE ###
  \end{flalign*}

  To prove that $H$ is positive semi-definite, show $z^T Hz \ge 0$ for all $z\in\Re^\di$.
  
  \begin{flalign*}
    z^T H z =
    % ### START CODE HERE ###
    z^T \left( \frac{1}{n} \sum_{i=1}^{n} h_{\theta}(x^{(i)})(1 - h_{\theta}(x^{(i)})) x^{(i)} x^{(i)T}\right) z =  \\ = \frac{1}{n} \sum_{i=1}^{n} h_{\theta}(x^{(i)})(1 - h_{\theta}(x^{(i)})) (z^T x^{(i)} x^{(i)T} z) = \\ = \frac{1}{n} \sum_{i=1}^{n} h_{\theta}(x^{(i)})(1 - h_{\theta}(x^{(i)})) (z^T x^{(i)})^2
    % ### END CODE HERE ###
  \end{flalign*}
  
  % ### START CODE HERE ###
  The scalar $(z^T x^{(i)})^2 \ge 0$, and $h^{(i)}(1-h^{(i)}) \ge 0$, therefore for $\forall z \in \mathbb{R}^{d}$, $z^{T}Hz \ge 0$ and $H \succeq 0$.
  % ### END CODE HERE ###
\end{answer}
% <SCPD_SUBMISSION_TAG>_1a
\clearpage

\LARGE
1.c
\normalsize

% <SCPD_SUBMISSION_TAG>_1c
\begin{answer}
  For shorthand, we let $\mc{H} = \{\phi, \Sigma, \mu_{0}, \mu_1\}$ denote the parameters for the problem. Since the given formulae are conditioned on $y$, use Bayes rule to get:
  \begin{align*}
    p(y =1\vert  x ; \mc{H}) = \frac {p(x\vert y=1; \mc{H}) p(y=1; \mc{H})} {p(x; \mc{H})}
    = \frac {p(x\vert y=1; \mc{H}) p(y=1; \mc{H})}
      {p(x\vert y=1; \mc{H}) p(y=1; \mc{H}) + p(x\vert y={0}; \mc{H}) p(y={0}; \mc{H})} = \\
      = \frac{\phi \exp(-\frac{1}{2}(x-\mu_1)^T \Sigma^{-1}(x-\mu_1))}{\phi \exp(-\frac{1}{2}(x-\mu_1)^T \Sigma^{-1}(x-\mu_1)) + (1-\phi) \exp(-\frac{1}{2}(x-\mu_0)^T \Sigma^{-1}(x-\mu_0))} = \\ = \frac{1}{1 + \frac{(1-\phi)}{\phi} \exp(\frac{1}{2}((x - \mu_1)^T\Sigma^{-1}(x - \mu_1) - (x - \mu_0)^T \Sigma^{-1}(x-\mu_0))}
  \end{align*}
  Let's simplify the expression in exponent, assuming $\Sigma$ is symmetric and positive definite and therefore $\Sigma^{-1}$ is also symmetric:
  \begin{align*}
  \frac{1}{2}(x-\mu_1)^T \Sigma^{-1}(x-\mu_1) - (x-\mu_0)^T \Sigma^{-1}(x-\mu_0) = \frac{1}{2}(\cancel{x^T \Sigma^{-1}x} - 2x^T \Sigma^{-1}\mu_1 + \mu_1^T \Sigma \mu_1 - \cancel{x^T \Sigma^{-1} x} + 2x^T \Sigma^{-1} \mu_0 - \mu_0^T \Sigma^{-1} \mu_0) = \\ = -x^T \Sigma^{-1} (\mu_1 - \mu_0) + \frac{1}{2}(\mu_1^T \Sigma^{-1} \mu_1 - \mu_0^T \Sigma^{-1} \mu_0) = 
  \end{align*}

  Let's set $\theta = \Sigma^{-1}(\mu_1 - \mu_0)$ and $\theta_0 = -\frac{1}{2}\mu_1^T \Sigma^{-1}\mu_1 + \frac{1}{2} \mu_0^T \Sigma^{-1}\mu_0 + \log (\frac{\phi}{1-\phi}) = \log(\frac{\phi}{1-\phi})-\frac{1}{2}(\mu_1+\mu_0)^T\Sigma^{-1}(\mu_1-\mu_0)$, so the final expression will be:
  \begin{align*}
  p(y=1\vert x) = \frac{1}{1 + \exp (-(\theta^T x + \theta_0))}
  \end{align*}
\end{answer}
% <SCPD_SUBMISSION_TAG>_1c
\clearpage

\LARGE
1.d
\normalsize

% <SCPD_SUBMISSION_TAG>_1d
\begin{answer}
  First, derive the expression for the log-likelihood of the training data:
  \begin{flalign*}
    \ell(\phi, \mu_{0}, \mu_1, \Sigma) &= \log \prod_{i=1}^\nexp p(x^{(i)} \vert  y^{(i)}; \mu_{0}, \mu_1, \Sigma) p(y^{(i)}; \phi)\\
    &= \sum_{i=1}^{\nexp} \log p(x^{(i)} \vert  y^{(i)}; \mu_{0}, \mu_1, \Sigma) +
    \sum_{i=1}^{n} \log p(y^{(i)}; \phi) = \\
    % ### START CODE HERE ###
    &= \sum_{i=1}^{\nexp} \left( y^{(i)}\log p(x^{(i)}\vert  y^{(i)}=1; \mu_{0}, \mu_1, \Sigma) + (1-y^{(i)}) \log p(x^{(i)}\vert  y^{(i)}=0; \mu_{0}, \mu_1, \Sigma)\right) + \\
    & \sum_{i=1}^{\nexp} \left( y^{(i)}\log \phi + (1-y^{(i)}) \log(1-\phi)\right)
    % ### END CODE HERE ###
  \end{flalign*}

  Now, the likelihood is maximized by setting the derivative (or gradient) with respect to each of the parameters to zero.\\

  \textbf{For $\mathbf{\phi}$:}

  \begin{flalign*}
    \frac{\partial \ell}{\partial \phi} =
    % ### START CODE HERE ###
    \sum_{i=1}^{\nexp} \frac{\partial (y^{(i)}\log \phi + (1-y^{(i)}) \log(1-\phi))} {\partial \phi} = \sum_{i=1}^{\nexp} \left( \frac{y^{(i)}}{\phi} - \frac{(1 - y^{(i)})}{(1 - \phi)} \right)
    % ### END CODE HERE ###
  \end{flalign*}

  Setting this equal to zero and solving for $\phi$ gives the maximum
  likelihood estimate.\\

  \begin{flalign*}
    \sum_{i=1}^{\nexp} \left( \frac{y^{(i)}}{\phi} - \frac{(1 - y^{(i)})}{(1 - \phi)} \right) = \frac{1}{\phi} \sum_{i=1}^{\nexp}y^{(i)} - \frac{1}{1 - \phi} \sum_{i=1}^{\nexp}(1 - y^{(i)}) = 0
  \end{flalign*}

  Let's assume $n$ is a total number of $y^{(i)}$ in the dataset, $n_0$ is a number of $y^{(i)}=0$ and $n_1$ is a number of $y^{(i)}=1$, $n_1 = \sum_{i=1}^{\nexp} 1 \{ y^{(i)} = 1 \}$, $n=n_0+n_1$, then
  \begin{flalign*}
    \frac{n_1}{\phi} - \frac{n_0}{(1 - \phi)} = \frac{n_1}{\phi} - \frac{n - n_1}{(1 - \phi)} = 0
  \end{flalign*}

  \begin{flalign*}
     n_1(1-\phi)=(n-n_1)\phi
  \end{flalign*}

  \begin{flalign*}
     n_1-n_1\phi=n\phi-n_1\phi
  \end{flalign*}

  \begin{flalign*}
     \phi=\frac{n_1}{n} = \frac{1}{n} \sum_{i=1}^{\nexp}y^{(i)} = \frac{1}{n} \sum_{i=1}^{\nexp} 1 \{ y^{(i)} = 1 \}
  \end{flalign*}

  \textbf{For $\mathbf{\mu_0}$:}

  {\bf Hint:}  Remember that $\Sigma$ (and thus $\Sigma^{-1}$) is symmetric. Let's skip part of $\ell$ depending on $\phi$. Also, the part that depend on $\mu_1$ is crossed, since in it there are no components depending on $\mu_0$:

  \begin{flalign*}
    \nabla_{\mu_{0}}\ell &= 
    % ### START CODE HERE ###
    \nabla_{\mu_{0}} \sum_{i=1}^{\nexp} \left( y^{(i)}\log p(x^{(i)}\vert  y^{(i)}=1; \mu_{0}, \mu_1, \Sigma) + (1-y^{(i)}) \log p(x^{(i)}\vert  y^{(i)}=0; \mu_{0}, \mu_1, \Sigma)\right) \\
    &= \cancel{\sum_{i=1}^{\nexp} y^{(i)} \nabla_{\mu_{0}}\log \left( \frac{\exp(-\frac{1}{2}(x^{(i)}-\mu_1)^T \Sigma^{-1}(x^{(i)}-\mu_1))}{(2\pi)^{d/2} \vert \Sigma \vert^{1/2}} \right)} +\\
    &+ \sum_{i=1}^{\nexp} (1 - y^{(i)}) \nabla_{\mu_{0}} \log \left( \frac{\exp(-\frac{1}{2}(x^{(i)}-\mu_0)^T \Sigma^{-1}(x^{(i)}-\mu_0))}{(2\pi)^{d/2} \vert \Sigma \vert^{1/2}} \right)
    % ### END CODE HERE ###
  \end{flalign*}
  Let's further simplify it:
  \begin{flalign*}
    \nabla_{\mu_0} \ell &= \sum_{i=1}^{\nexp} (1 - y^{(i)}) \nabla_{\mu_{0}} \left( -\frac{1}{2}(x^{(i)}-\mu_0)^T \Sigma^{-1}(x^{(i)}-\mu_0) - \frac{d}{2}\log (2\pi) -\frac{1}{2}\log{\vert \Sigma \vert}) \right) = \\
    &= \sum_{i=1}^{\nexp} (1 - y^{(i)}) \nabla_{\mu_{0}} \left( -\frac{1}{2}(x^{(i)}-\mu_0)^T \Sigma^{-1}(x^{(i)}-\mu_0) \right) = \\
    &= \sum_{i=1}^{\nexp} (1 - y^{(i)}) \Sigma^{-1}(x^{(i)}-\mu_0)
    % ### END CODE HERE ###
  \end{flalign*}

  Since $(1-y^{(i)})$ is a scalar, and $\Sigma^{-1}$ is a constant symmetric matrix, we could re-write the result as:

  \begin{flalign*}
    \nabla_{\mu_0} \ell &= \Sigma^{-1} \sum_{i=1}^{\nexp} (1 - y^{(i)}) (x^{(i)}-\mu_0)
  \end{flalign*}

  Setting this gradient to zero gives the maximum likelihood estimate for $\mu_{0}$.\\

  \begin{flalign*}
    & \sum_{i=1}^{\nexp} (1 - y^{(i)}) (x^{(i)}-\mu_0) = 0 \\
    & \sum_{i=1}^{\nexp} 1 \{y^{(i)}=0 \} \mu_0 = \sum_{i=1}^{\nexp} 1 \{y^{(i)}=0 \} x^{(i)} \\
    & \mu_0 = \frac{\sum_{i=1}^{\nexp} 1 \{y^{(i)}=0 \} x^{(i)}}{\sum_{i=1}^{\nexp} 1 \{y^{(i)}=0 \}}
  \end{flalign*}

  \textbf{For $\mathbf{\mu_1}$:}

  {\bf Hint:}  Remember that $\Sigma$ (and thus $\Sigma^{-1}$) is symmetric. Similar reasoning as above works for $\nabla_{\mu_1} \ell$ with only difference that it's valid for $y^{(i)}=1$ instead of $y^{(i)}=0$:

  \begin{flalign*}
    \nabla_{\mu_{1}}\ell &= \Sigma^{-1} \sum_{i=1}^{\nexp} y^{(i)} (x^{(i)}-\mu_1)
  \end{flalign*}

  Setting this gradient to zero gives the maximum likelihood estimate
  for $\mu_{1}$.\\

  \begin{flalign*}
    & \sum_{i=1}^{\nexp} y^{(i)} (x^{(i)}-\mu_1) = 0 \\
    & \sum_{i=1}^{\nexp} 1 \{y^{(i)}=1\} \mu_1 = \sum_{i=1}^{\nexp} 1 \{y^{(i)}=1 \} x^{(i)} \\
    & \mu_1 = \frac{\sum_{i=1}^{\nexp} 1 \{y^{(i)}=1 \} x^{(i)}}{\sum_{i=1}^{\nexp} 1 \{y^{(i)}=1 \}}
  \end{flalign*}

  For $\Sigma$, we find the gradient with respect to $S = \Sigma^{-1}$ rather than $\Sigma$ just to simplify the derivation (note that $\vert S\vert  = \frac{1}{\vert \Sigma\vert }$).
  You should convince yourself that the maximum likelihood estimate $S_\nexp$ found in this way would correspond to the actual maximum likelihood estimate $\Sigma_\nexp$ as $S_\nexp^{-1} = \Sigma_\nexp$.

  {\bf Hint:}  You may need the following identities: 
  \begin{equation*}
    \nabla_S \vert S\vert  = \vert S\vert  (S^{-1})^T
  \end{equation*}
  \begin{equation*}
    \nabla_S b_i^T S b_i = \nabla_S tr \left( b_i^T S b_i \right) =
    \nabla_S tr \left( S b_i b_i^T \right) = b_i b_i^T
  \end{equation*}

  \begin{flalign*}
    \nabla_S\ell &= \sum_{i=1}^{\nexp} 1 \{y^{(i)}=0\} \left( \nabla_{S}(-\frac{1}{2}(x^{(i)}-\mu_0)^T S (x^{(i)}-\mu_0))-\nabla_S \frac{d}{2}\log(2\pi) + \frac{1}{2}\nabla_S\log \vert S \vert \right) + \\
    &+ \sum_{i=1}^{\nexp} 1 \{y^{(i)}=1\} \left( \nabla_{S}(-\frac{1}{2}(x^{(i)}-\mu_1)^T S (x^{(i)}-\mu_1))-\nabla_S \frac{d}{2}\log(2\pi) + \frac{1}{2}\nabla_S\log \vert S \vert \right) = \\
    &= \sum_{i=1}^{\nexp} 1 \{y^{(i)}=0\} \left( -\frac{1}{2}(x^{(i)}-\mu_0) (x^{(i)}-\mu_0)^T + \frac{\vert S \vert (S^{-1})}{2 \vert S \vert} \right) + \\
    &+ \sum_{i=1}^{\nexp} 1 \{y^{(i)}=1\} \left( -\frac{1}{2}(x^{(i)}-\mu_1) (x^{(i)}-\mu_1)^T + \frac{\vert S \vert (S^{-1})}{2 \vert S \vert} \right) = \\
    &= \frac{n}{2}S^{-1} - \frac{1}{2} \sum_{i=1}^{\nexp} (x^{(i)}-\mu_{y^{(i)}}) (x^{(i)}-\mu_{y^{(i)}})^T
    % ### START CODE HERE ###
  % ### END CODE HERE ###
  \end{flalign*}

  Next, substitute $\Sigma = S^{-1}$.  Setting this gradient to zero gives the required maximum likelihood estimate for $\Sigma$.\\
  \begin{flalign*}
    \frac{n}{2}S^{-1} = \frac{1}{2} \sum_{i=1}^{\nexp} (x^{(i)}-\mu_{y^{(i)}}) (x^{(i)}-\mu_{y^{(i)}})^T \\
    \Sigma = \frac{1}{n} \sum_{i=1}^{\nexp} (x^{(i)}-\mu_{y^{(i)}}) (x^{(i)}-\mu_{y^{(i)}})^T
  \end{flalign*}
\end{answer}
% <SCPD_SUBMISSION_TAG>_1d
\clearpage

\LARGE
1.f
\normalsize

% <SCPD_SUBMISSION_TAG>_1f
\begin{answer}
  % ### START CODE HERE ###
  For dataset 1 we can see that GDA separates the classes with an almost vertical line. It's not very effective. Logistic regression line better separates the classes. In the plot the classes are distributed in a non-gaussian way and there are outliers, that distort mean and covariance estimates. For such distribution, logistic regression is a more appropriate choice.
  % ### END CODE HERE ###
\end{answer}
% <SCPD_SUBMISSION_TAG>_1f
\clearpage

\LARGE
1.g
\normalsize

% <SCPD_SUBMISSION_TAG>_1g
\begin{answer}
  % ### START CODE HERE ###
  For dataset 2 GDA performs much better, similar to logistic regression. This is a consequence of data distribution as in dataset 2 the fields are roughly equal in area, clearly separated - have small intersection.
  % ### END CODE HERE ###
\end{answer}
% <SCPD_SUBMISSION_TAG>_1g
\clearpage

\LARGE
1.h
\normalsize

% <SCPD_SUBMISSION_TAG>_1h
\begin{answer}
  % ### START CODE HERE ###
  If we make $x_{(i)}$ to be distributed more normally in dataset 1 and eliminate outliers, then GDA will work better.
  % ### END CODE HERE ###
\end{answer}
% <SCPD_SUBMISSION_TAG>_1h
\clearpage

\LARGE
2.c
\normalsize

% <SCPD_SUBMISSION_TAG>_2c
\begin{answer}
  
  Let's use Bayes theorem:
  \begin{flalign*}
  % ### START CODE HERE ###
  p(t=1 \vert y = 1, x) &= \frac{p(y=1 \vert t=1, x)\ p(t=1 \vert x)}{p(y=1 \vert x)}
  % ### END CODE HERE ###
  \end{flalign*}

  We assume that if $t=0$ then $y=1$ never occurs:
  \begin{flalign*}
  p(y=1 \vert t=0,x)=0
  \end{flalign*}

  We also assume that if $t=1$ then $y=1$ with some non zero probability. Rewriting the denominator of the first formula:
  \begin{flalign*}
  p(y=1 \vert x) &= p(y = 1 \vert t=1, x)\ p(t=1 \vert x) + p(y=1 \vert t=0, x)\ p(t=0 \vert x) = \\
  &= p(y=1 \vert t=1, x)\ p(t =1 \vert x) + 0 \cdot p(t=0 \vert x) = \\
  &= p(y=1 \vert t = 1, x)\ p(t=1 \vert x)
  \end{flalign*}

  Therefore the first formula becomes:
  \begin{flalign*}
  p(t=1 \vert y = 1, x) &= \frac{p(y=1 \vert t=1, x)\ p(t=1 \vert x)}{p(y=1 \vert x)} = \frac{p(y=1 \vert t=1, x)\ p(t=1 \vert x)}{p(y=1 \vert t = 1, x)\ p(t=1 \vert x)} = 1
  \end{flalign*}
\end{answer}
% <SCPD_SUBMISSION_TAG>_2c
\clearpage

\LARGE
2.d
\normalsize

% <SCPD_SUBMISSION_TAG>_2d
\begin{answer}
  % ### START CODE HERE ###
  Assuming $p(y=1 \vert x) = \alpha\ p(t=1 \vert x)$ and $p(y=1 \vert t = 0, x) = 0$ let's use the law of total probability:
  \begin{flalign*}
  p(y=1 \vert x) &= p(y=1 \vert t=1, x)\ p(t=1 \vert x) + p(y=1 \vert t = 0, x)\ p(t=0 \vert x) = \\
  &= \alpha\ p(t=1 \vert x) + 0 \cdot p(t=0 \vert x) = \\
  &= \alpha\ p(t=1 \vert x)
  \end{flalign*}
  Therefore:
  \begin{flalign*}
  p(t=1 \vert x) = \frac{1}{\alpha}\ p(y=1 \vert x) 
  \end{flalign*}
  % ### END CODE HERE ###
\end{answer}
% <SCPD_SUBMISSION_TAG>_2d
\clearpage

\LARGE
2.e
\normalsize

% <SCPD_SUBMISSION_TAG>_2e
\begin{answer}
  We have as an input:
  \begin{flalign*}
  h(x) = p(y = 1 \vert x)
  \end{flalign*}
  We also know that:
  \begin{flalign*}
  p(y=1 \vert x) = \alpha\ p(t=1\vert x)
  \end{flalign*}
  Therefore:
  \begin{flalign*}
  h(x) = \alpha\ p(t=1 \vert x)
  \end{flalign*}

  Our assumption is that $p(t^{(i)}=1 \vert x^{(i)}) \in \{0, 1\}$, therefore:

  \begin{equation}
      h(x)=\begin{cases}
          \alpha, & \text{if $t=1$}\\
          0, & \text{if $t=0$}
      \end{cases}
  \end{equation}
  
  Let's compute $\mathbb{E}$
  \begin{equation}
      \mathbb{E}[h(x) \vert y =1] = \int h(x)\ p(x \vert y = 1) dx
  \end{equation}

  taking (1) into account, the expression (2) becomes the average of $\alpha$ over $x$ points where $y=1$, therefore:
  \begin{flalign*}
      \mathbb{E}[h(x) \vert y=1] = \alpha\ p(t=1 \vert y=1)
  \end{flalign*}

  From previous result $p(t=1\vert y=1, x)=1$, consequently:
  \begin{flalign*}
      \mathbb{E}[h(x) \vert y=1] = \alpha\cdot 1 = \alpha
  \end{flalign*}

  Finally
  \begin{flalign*}
    \alpha = \mathbb{E}[h(x^{(i)}) \vert y^{(i)}=1]
  \end{flalign*}

  If we assume $V_+$ as a set of examples where $y^{(i)}=1$, then:
  \begin{flalign*}
    \alpha \approx \frac{1}{\vert V_+ \vert} \sum_{x^{(i)} \in V_+} h(x^{(i)})
  \end{flalign*}
  
\end{answer}
% <SCPD_SUBMISSION_TAG>_2e
\clearpage

\LARGE
3.ai
\normalsize

% <SCPD_SUBMISSION_TAG>_3ai
\begin{answer}
  % ### START CODE HERE ###
  Let's create a trivial classifier $f(x)$ that always predicts the negative class label 0, regardless of $x$. It correctly classifies all the negative examples with label 0 but incorrectly classifies positive examples with label 1. The dataset has $\rho$ positive examples and $1-\rho$ negative examples. There are $n$ examples in the dataset. So:
  \begin{flalign*}
      A = \frac{\text{\# correct predictions}}{\text{\# all examples}} = \frac{(1 - \rho) \cdot n}{n} = 1 - \rho
  \end{flalign*}
  % ### END CODE HERE ###
\end{answer}
% <SCPD_SUBMISSION_TAG>_3ai

\LARGE
3.aii
\normalsize

% <SCPD_SUBMISSION_TAG>_3aii
\begin{answer}
  We have the following definition of accuracy:
  \begin{equation}
      A = \frac{TP + TN}{TP + TN + FP + FN}
  \end{equation}
      

  The number of positive examples in the data set could be expressed as $TP + FN$. We sum up only the positive examples. The following subsets make up all the positive examples in the dataset:
  \begin{flalign*}
      TP &\triangleq \#\ \text{positive examples with a correct (positive) prediction} \\
      FN &\triangleq \#\ \text{positive examples with a incorrect (negative) prediction}
  \end{flalign*}

  The overall amount of examples is expressed as $TP + TN + FP + FN = n$.
  The number of positive examples $n_+$ could be expressed as follows:
  \begin{flalign*}
      \#\ \text{positive examples} = n_+ = \rho\ n = \rho\ (TP + TN + FP + FN)
  \end{flalign*}

  From the other side:
  \begin{flalign*}
      n_+ = TP + FN
  \end{flalign*}

  Therefore:
  \begin{flalign*}
      n_+ &= TP + FN = \rho\ (TP + TN + FP + FN) \\
      \rho &= \frac{TP + FN}{TP + TN + FP + FN}
  \end{flalign*}
  Similarly to that, let's write the formula for \# negative examples, $n_-$:
  \begin{flalign*}
      n_- &= TN + FP = (1 -\rho)\ (TP + TN + FP + FN) \\
      1-\rho &= \frac{TN + FP}{TP + TN + FP + FN}
  \end{flalign*}

  We take into account that:
  \begin{flalign*}
      A_0 &= \frac{TN}{TN + FP} => TN = A_0(TN + FP)\\
      A_1 &= \frac{TP}{TP + FN} => TP = A_1(TP + FN)
  \end{flalign*}

  Let's write the expression for A taking these formulae into account, also remember expressions with $\rho$ and $1-\rho$ written earlier:
  \begin{flalign*}
      A &= \frac{TP + TN}{TP + TN + FP + FN} = \frac{A_1(TP + FN) + A_0(TN + FP)}{TP + TN + FP + FN} = \\
      &= \frac{A_1\rho (TP+TN+FP+FN) + A_0(1-\rho)(TP+TN+FP+FN)}{TP+TN+FP+FN} = \\ &= A_1\rho+A_0(1-\rho)
  \end{flalign*}
\end{answer}
% <SCPD_SUBMISSION_TAG>_3aii

\LARGE
3.aiii
\normalsize

% <SCPD_SUBMISSION_TAG>_3aiii
\begin{answer}
  We define the balanced accuracy as:
  \begin{flalign*}
    \overline{A} = \frac{1}{2}(A_0 + A_1)
  \end{flalign*}

  The mentioned trivial classifier predicts 0 for any input $x$, therefore we correctly predict all negative examples, therefore the following equalities for negative examples are correct:
  \begin{flalign*}
    FN &= n_+ = \#\ \text{positive examples predicted as negative}\\
    TN &= n_- = \#\ \text{negative examples} 
  \end{flalign*}

  For positive part of the dataset we have the following:
  \begin{flalign*}
    FP &=0 \\
    TP &=0
  \end{flalign*}
  
  Therefore, provided that there is at least one negative example in the dataset, the balanced accuracy becomes:
  \begin{flalign*}
    \overline{A} = \frac{1}{2}(A_0 + A_1) = \frac{1}{2}(\frac{TN}{TN+FP} + \frac{TP}{TP+FN}) = \frac{1}{2}(\frac{TN}{TN + 0}+\frac{0}{0 + FN}) = \frac{1}{2}(1 + 0) = \frac{1}{2}
  \end{flalign*}
\end{answer}
% <SCPD_SUBMISSION_TAG>_3aiii

\LARGE
3.c
\normalsize

% <SCPD_SUBMISSION_TAG>_3c
\begin{answer}
  The balanced accuracy definition:
  \begin{flalign*}
       \overline{A} = \frac{1}{2}(A_0 + A_1)
  \end{flalign*}

  We assume that $\rho < \frac{1}{2}$ and $\kappa = \frac{\rho}{1-\rho}$, $\frac{1}{\kappa} \in \mathbb{Z}$

  Let's write the expressions for positive, negative examples and $\kappa$ in the dataset $D$ as:
  \begin{flalign*}
      \#\text{positive examples}=n_+ &= \rho\ n \\
      \#\text{negative examples}=n_- &= (1 - \rho)\ n \\
      \kappa = \frac{\rho}{1-\rho} = \frac{n_+}{n_-}
  \end{flalign*}

  Let's create dataset $D^{'}$ as:
    \begin{flalign*}
      \#\text{negative examples in dataset}\ D'=n_-^{'} &=n_- = (1 - \rho)\ n \\
      \#\text{positive examples in dataset}\ D'=n_+^{'} &= \frac{n_+}{\kappa} =\frac{\rho\ n}{\kappa} = n_-
  \end{flalign*}

  Overall amount of entries in dataset $D'$ equals to $2n_-$ examples with $n_-$ negative examples and $n_-$ positives examples.

  Taking this into account, let's write the balanced accuracy for dataset $D$:
  \begin{flalign*}
    n_+ &= TP + FN\\
    n_- &= TN + FP \\
    \overline{A} &= \frac{1}{2}(\frac{TP}{n_=} + \frac{TN}{n_+})
  \end{flalign*}

  Let's write the balanced accuracy $A^{'}$ for dataset $D'$, where $TP'$ and $FN'$ are true positives and false negatives for dataset $D'$:
  \begin{flalign*}
    n_+^{'} &= n_- = TP' + FN'\\
    n_-^{'} &= n_- = TN + FP \\
    TP' &= \frac{TP}{\kappa}\\
    \overline{A^{'}} &= \frac{1}{2}(\frac{TN}{n_-^{'}} + \frac{TP'}{n_+^{'}}) = \frac{1}{2}(\frac{TN}{n_-} + \frac{TP}{\kappa\ n_-}) = \frac{1}{2}(\frac{TN}{n_-} + \frac{TP}{n_+}) = \overline{A}
  \end{flalign*}

  Let's derive the average empirical loss for logistic regression on dataset $D'$. The loss per sample:
  \begin{flalign*}
      \ell(x^{(i)}, y^{(i)}) = -[y^{(i)}\log h_\theta(x^{(i)}) + (1-y^{(i)})\log(1-h_\theta(x^{(i)})]
  \end{flalign*}

  The total loss $J_{total}(\theta)$:
  \begin{flalign*}
      J_{total}(\theta) = \sum_{i=1}^{n}\ell(x^{(i)}, y^{(i)}) = \sum_{i=1,y^{(i)}=0}^{n_-}\ell(x^{(i)}, y^{(i)}) + \frac{1}{\kappa}\sum_{i=1,y^{(i)}=1}^{n_-}\ell(x^{(i)}, y^{(i)})
  \end{flalign*}

  Given that:
  \begin{flalign*}
      \omega^{(i)}=\begin{cases}
          1, & \text{if $y^{(i)}=0$}\\
          \frac{1}{\kappa}, & \text{if $y^{(i)}=1$}
      \end{cases}
  \end{flalign*}

  The total loss becomes:
  \begin{flalign*}
      J_{total}(\theta) = \sum_{i=1}^{n}\omega^{(i)}\ell(x^{(i)}, y^{(i)})
  \end{flalign*}

  The average loss $\overline{L}$ will be:
  \begin{flalign*}
      J(\theta) = \frac{1}{n}\sum_{i=1}^{n}\omega^{(i)}\ell(x^{(i)}, y^{(i)})
  \end{flalign*}

  As we know, $n_-=(1-\rho)n$ and $\kappa=\frac{\rho}{1-\rho}$. Let's make an expression for $\rho(\kappa)$:
  \begin{flalign*}
      \kappa = \frac{\rho}{1-\rho} \\
      \kappa-\kappa\rho - \rho = 0 \\
      \kappa = \rho(\kappa + 1) \\
      \rho = \frac{\kappa}{\kappa + 1}
  \end{flalign*}

  Therefore, taking into account that $n=2n_-=2(1-\rho)n$
  \begin{flalign*}
      J(\theta) &= \frac{1}{2n(1-\rho)}\sum_{i=1}^{n}\omega^{(i)}\ell(x^{(i)}, y^{(i)}) = \frac{1}{2n(1-\frac{k}{k+1})}\sum_{i=1}^{n}\omega^{(i)}\ell(x^{(i)}, y^{(i)}) = \\
      &=\frac{k+1}{2n}\sum_{i=1}^{n}\omega^{(i)}\ell(x^{(i)}, y^{(i)}) = \frac{k+1}{2n}\sum_{i=1}^{n}\omega^{(i)}\ell(x^{(i)}, y^{(i)}) = \\
      &= -\frac{k+1}{2n}\sum_{i=1}^{n}\omega^{(i)}[y^{(i)}\log h_\theta(x^{(i)}) + (1-y^{(i)})\log(1-h_\theta(x^{(i)})]
  \end{flalign*}
\end{answer}


\end{document}
